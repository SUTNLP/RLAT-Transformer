{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHrfbpiAtjMT",
        "outputId": "a04d53a5-7d97-4a1e-81d6-25b66cdfec49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-26 14:34:14--  https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3846925 (3.7M) [text/plain]\n",
            "Saving to: ‘edos_labelled_aggregated.csv’\n",
            "\n",
            "\r          edos_labe   0%[                    ]       0  --.-KB/s               \redos_labelled_aggre 100%[===================>]   3.67M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-02-26 14:34:14 (47.1 MB/s) - ‘edos_labelled_aggregated.csv’ saved [3846925/3846925]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (23.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (6.0.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4d9a09f1dc1642dd4a538a28e840bfd44db5a759808223d7fc2203a20479b98c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\n",
            "To: /content/train_data.zip\n",
            "100% 87.1M/87.1M [00:00<00:00, 196MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
            "To: /content/eval_A.zip\n",
            "100% 125k/125k [00:00<00:00, 82.7MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
            "To: /content/eval_B.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 47.1MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
            "To: /content/eval_C.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 36.9MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
            "To: /content/gab_1M_unlabelled.csv\n",
            "100% 95.9M/95.9M [00:00<00:00, 184MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
            "To: /content/reddit_1M_unlabelled.csv\n",
            "100% 98.0M/98.0M [00:00<00:00, 209MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
            "To: /content/good.pk\n",
            "100% 1.84M/1.84M [00:00<00:00, 201MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
            "To: /content/dev_task_b_labels.csv\n",
            "100% 20.4k/20.4k [00:00<00:00, 33.1MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
            "To: /content/dev_task_c_labels.csv\n",
            "100% 33.5k/33.5k [00:00<00:00, 34.7MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EKFujUoPzVspdwoNSFsLvw9HBOUMZ1ys\n",
            "To: /content/test_task_b_entries.csv\n",
            "100% 157k/157k [00:00<00:00, 114MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oDHszlQtjfYOXpe6_v3zdptm5al2FQkl\n",
            "To: /content/test_task_c_entries.csv\n",
            "100% 157k/157k [00:00<00:00, 78.8MB/s]\n",
            "Archive:  train_data.zip\n",
            "  inflating: train_all_tasks.csv     \n",
            "replace reddit_1M_unlabelled.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: reddit_1M_unlabelled.csv  \n",
            "  inflating: gab_1M_unlabelled.csv   \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_a.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_b.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_c.zip  \n",
            "Archive:  eval_A.zip\n",
            "  inflating: dev_task_a_entries.csv  \n",
            "Archive:  eval_B.zip\n",
            "  inflating: dev_task_b_entries.csv  \n",
            "Archive:  eval_C.zip\n",
            "  inflating: dev_task_c_entries.csv  \n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
        "! pip install transformers\n",
        "! pip install optuna\n",
        "! pip install sentencepiece\n",
        "! pip install gdown\n",
        "! pip install sentence-transformers\n",
        "\n",
        "! gdown \"1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\"\n",
        "! gdown --id 16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
        "! gdown --id 1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
        "! gdown --id 11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
        "! gdown --id 1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
        "! gdown --id 1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
        "! gdown --id 1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
        "! gdown --id 1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
        "! gdown --id 1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
        "\n",
        "! gdown --id 1EKFujUoPzVspdwoNSFsLvw9HBOUMZ1ys\n",
        "! gdown --id 1oDHszlQtjfYOXpe6_v3zdptm5al2FQkl\n",
        "\n",
        "! yes A | unzip train_data.zip\n",
        "! unzip eval_A.zip\n",
        "! unzip eval_B.zip\n",
        "! unzip eval_C.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtsdz5OStx0R",
        "outputId": "b74ff332-9547-4b94-c948-7e0cf162c655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-26 14:35:17.554058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-26 14:35:18.562046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 14:35:18.562167: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 14:35:18.562185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 7.61kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 580/580 [00:00<00:00, 87.8kB/s]\n",
            "Downloading (…)\"spm.model\";: 100% 2.46M/2.46M [00:00<00:00, 30.5MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 874M/874M [00:04<00:00, 175MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "EPOCH [1/30] | STEP [100/340] | CE Loss 1.3968\n",
            "EPOCH [1/30] | STEP [100/340] | ADV Loss 1.4043\n",
            "EPOCH [1/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | STEP [200/340] | CE Loss 1.356\n",
            "EPOCH [1/30] | STEP [200/340] | ADV Loss 1.3909\n",
            "EPOCH [1/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | STEP [300/340] | CE Loss 1.2256\n",
            "EPOCH [1/30] | STEP [300/340] | ADV Loss 1.3226\n",
            "EPOCH [1/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | Current F1-Macro 36.89\n",
            "EPOCH [1/30] | Best F1-Macro 36.89\n",
            "[[ 38   0   2   4]\n",
            " [ 16   1 119  91]\n",
            " [  7   0 103  57]\n",
            " [  4   0  14  30]]\n",
            "not early stopping\n",
            "EPOCH [2/30] | STEP [100/340] | CE Loss 1.1153\n",
            "EPOCH [2/30] | STEP [100/340] | ADV Loss 1.2223\n",
            "EPOCH [2/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | STEP [200/340] | CE Loss 1.0799\n",
            "EPOCH [2/30] | STEP [200/340] | ADV Loss 1.2336\n",
            "EPOCH [2/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | STEP [300/340] | CE Loss 0.9449\n",
            "EPOCH [2/30] | STEP [300/340] | ADV Loss 1.1285\n",
            "EPOCH [2/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | Current F1-Macro 61.21\n",
            "EPOCH [2/30] | Best F1-Macro 61.21\n",
            "[[ 42   1   1   0]\n",
            " [ 27 120  73   7]\n",
            " [ 17  26 112  12]\n",
            " [ 10   5   5  28]]\n",
            "not early stopping\n",
            "EPOCH [3/30] | STEP [100/340] | CE Loss 0.704\n",
            "EPOCH [3/30] | STEP [100/340] | ADV Loss 0.9233\n",
            "EPOCH [3/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | STEP [200/340] | CE Loss 0.7345\n",
            "EPOCH [3/30] | STEP [200/340] | ADV Loss 0.9716\n",
            "EPOCH [3/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | STEP [300/340] | CE Loss 0.7108\n",
            "EPOCH [3/30] | STEP [300/340] | ADV Loss 0.9098\n",
            "EPOCH [3/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | Current F1-Macro 64.54\n",
            "EPOCH [3/30] | Best F1-Macro 64.54\n",
            "[[ 38   1   2   3]\n",
            " [ 18 171  25  13]\n",
            " [ 11  64  78  14]\n",
            " [  3   8   2  35]]\n",
            "not early stopping\n",
            "EPOCH [4/30] | STEP [100/340] | CE Loss 0.4915\n",
            "EPOCH [4/30] | STEP [100/340] | ADV Loss 0.7131\n",
            "EPOCH [4/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | STEP [200/340] | CE Loss 0.4499\n",
            "EPOCH [4/30] | STEP [200/340] | ADV Loss 0.6536\n",
            "EPOCH [4/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | STEP [300/340] | CE Loss 0.4607\n",
            "EPOCH [4/30] | STEP [300/340] | ADV Loss 0.6688\n",
            "EPOCH [4/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | Current F1-Macro 64.72\n",
            "EPOCH [4/30] | Best F1-Macro 64.72\n",
            "[[ 39   3   0   2]\n",
            " [ 15 154  33  25]\n",
            " [  8  48  87  24]\n",
            " [  3   3   3  39]]\n",
            "not early stopping\n",
            "EPOCH [5/30] | STEP [100/340] | CE Loss 0.3234\n",
            "EPOCH [5/30] | STEP [100/340] | ADV Loss 0.5196\n",
            "EPOCH [5/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | STEP [200/340] | CE Loss 0.2965\n",
            "EPOCH [5/30] | STEP [200/340] | ADV Loss 0.4586\n",
            "EPOCH [5/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | STEP [300/340] | CE Loss 0.3184\n",
            "EPOCH [5/30] | STEP [300/340] | ADV Loss 0.4754\n",
            "EPOCH [5/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | Current F1-Macro 71.66\n",
            "EPOCH [5/30] | Best F1-Macro 71.66\n",
            "[[ 36   3   3   2]\n",
            " [  7 165  50   5]\n",
            " [  1  37 122   7]\n",
            " [  2  11   7  28]]\n",
            "not early stopping\n",
            "EPOCH [6/30] | STEP [100/340] | CE Loss 0.1859\n",
            "EPOCH [6/30] | STEP [100/340] | ADV Loss 0.3214\n",
            "EPOCH [6/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | STEP [200/340] | CE Loss 0.2387\n",
            "EPOCH [6/30] | STEP [200/340] | ADV Loss 0.4174\n",
            "EPOCH [6/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | STEP [300/340] | CE Loss 0.2033\n",
            "EPOCH [6/30] | STEP [300/340] | ADV Loss 0.3515\n",
            "EPOCH [6/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | Current F1-Macro 71.39\n",
            "EPOCH [6/30] | Best F1-Macro 71.66\n",
            "[[ 38   2   2   2]\n",
            " [  7 167  48   5]\n",
            " [  1  43 116   7]\n",
            " [  2  12   7  27]]\n",
            "not early stopping\n",
            "EPOCH [7/30] | STEP [100/340] | CE Loss 0.1054\n",
            "EPOCH [7/30] | STEP [100/340] | ADV Loss 0.227\n",
            "EPOCH [7/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | STEP [200/340] | CE Loss 0.1131\n",
            "EPOCH [7/30] | STEP [200/340] | ADV Loss 0.2337\n",
            "EPOCH [7/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | STEP [300/340] | CE Loss 0.1273\n",
            "EPOCH [7/30] | STEP [300/340] | ADV Loss 0.2483\n",
            "EPOCH [7/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | Current F1-Macro 72.54\n",
            "EPOCH [7/30] | Best F1-Macro 72.54\n",
            "[[ 37   4   1   2]\n",
            " [  5 192  24   6]\n",
            " [  0  64  95   8]\n",
            " [  2  12   4  30]]\n",
            "not early stopping\n",
            "EPOCH [8/30] | STEP [100/340] | CE Loss 0.0694\n",
            "EPOCH [8/30] | STEP [100/340] | ADV Loss 0.1719\n",
            "EPOCH [8/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | STEP [200/340] | CE Loss 0.0625\n",
            "EPOCH [8/30] | STEP [200/340] | ADV Loss 0.1755\n",
            "EPOCH [8/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | STEP [300/340] | CE Loss 0.0712\n",
            "EPOCH [8/30] | STEP [300/340] | ADV Loss 0.1974\n",
            "EPOCH [8/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | Current F1-Macro 69.96\n",
            "EPOCH [8/30] | Best F1-Macro 72.54\n",
            "[[ 35   4   3   2]\n",
            " [  5 176  41   5]\n",
            " [  0  55 102  10]\n",
            " [  2  12   6  28]]\n",
            "not early stopping\n",
            "EPOCH [9/30] | STEP [100/340] | CE Loss 0.0407\n",
            "EPOCH [9/30] | STEP [100/340] | ADV Loss 0.1422\n",
            "EPOCH [9/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | STEP [200/340] | CE Loss 0.0369\n",
            "EPOCH [9/30] | STEP [200/340] | ADV Loss 0.1554\n",
            "EPOCH [9/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | STEP [300/340] | CE Loss 0.0352\n",
            "EPOCH [9/30] | STEP [300/340] | ADV Loss 0.1416\n",
            "EPOCH [9/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | Current F1-Macro 71.75\n",
            "EPOCH [9/30] | Best F1-Macro 72.54\n",
            "[[ 36   3   2   3]\n",
            " [  8 172  40   7]\n",
            " [  1  42 114  10]\n",
            " [  2   9   6  31]]\n",
            "not early stopping\n",
            "EPOCH [10/30] | STEP [100/340] | CE Loss 0.0287\n",
            "EPOCH [10/30] | STEP [100/340] | ADV Loss 0.1295\n",
            "EPOCH [10/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | STEP [200/340] | CE Loss 0.0318\n",
            "EPOCH [10/30] | STEP [200/340] | ADV Loss 0.1177\n",
            "EPOCH [10/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | STEP [300/340] | CE Loss 0.02\n",
            "EPOCH [10/30] | STEP [300/340] | ADV Loss 0.095\n",
            "EPOCH [10/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | Current F1-Macro 70.2\n",
            "EPOCH [10/30] | Best F1-Macro 72.54\n",
            "[[ 37   3   2   2]\n",
            " [ 10 181  29   7]\n",
            " [  2  58  97  10]\n",
            " [  3  10   4  31]]\n",
            "not early stopping\n",
            "EPOCH [11/30] | STEP [100/340] | CE Loss 0.0126\n",
            "EPOCH [11/30] | STEP [100/340] | ADV Loss 0.0625\n",
            "EPOCH [11/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | STEP [200/340] | CE Loss 0.0169\n",
            "EPOCH [11/30] | STEP [200/340] | ADV Loss 0.1043\n",
            "EPOCH [11/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | STEP [300/340] | CE Loss 0.0196\n",
            "EPOCH [11/30] | STEP [300/340] | ADV Loss 0.0895\n",
            "EPOCH [11/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | Current F1-Macro 70.3\n",
            "EPOCH [11/30] | Best F1-Macro 72.54\n",
            "[[ 30   6   2   6]\n",
            " [  3 181  36   7]\n",
            " [  0  52 103  12]\n",
            " [  1   9   5  33]]\n"
          ]
        }
      ],
      "source": [
        "! python Task_10_B.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yx0Vi4ltly7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
        "! pip install transformers\n",
        "! pip install optuna\n",
        "! pip install sentencepiece\n",
        "! pip install gdown\n",
        "! pip install sentence-transformers\n",
        "\n",
        "! gdown \"1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\"\n",
        "! gdown --id 16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
        "! gdown --id 1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
        "! gdown --id 11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
        "! gdown --id 1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
        "! gdown --id 1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
        "! gdown --id 1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
        "! gdown --id 1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
        "! gdown --id 1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
        "\n",
        "! gdown \"1-8nY2nmkxEOYMSFLCuMUVPnILpFgsOfv&confirm=t\"\n",
        "\n",
        "! yes A | unzip train_data.zip\n",
        "! unzip eval_A.zip\n",
        "! unzip eval_B.zip\n",
        "! unzip eval_C.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN5SJiAKKRwi",
        "outputId": "47d4d309-a100-4d0a-93e5-97f781d60d4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-23 13:56:10--  https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3846925 (3.7M) [text/plain]\n",
            "Saving to: ‘edos_labelled_aggregated.csv’\n",
            "\n",
            "edos_labelled_aggre 100%[===================>]   3.67M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-02-23 13:56:10 (140 MB/s) - ‘edos_labelled_aggregated.csv’ saved [3846925/3846925]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (6.0.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=bcfb564cc7fc919e87cfd080e43c73140a16484ca01c39c99c4524bf03bc5945\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\n",
            "To: /content/train_data.zip\n",
            "100% 87.1M/87.1M [00:02<00:00, 34.0MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
            "To: /content/eval_A.zip\n",
            "100% 125k/125k [00:00<00:00, 81.2MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
            "To: /content/eval_B.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 53.3MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
            "To: /content/eval_C.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 52.2MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
            "To: /content/gab_1M_unlabelled.csv\n",
            "100% 95.9M/95.9M [00:00<00:00, 297MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
            "To: /content/reddit_1M_unlabelled.csv\n",
            "100% 98.0M/98.0M [00:00<00:00, 184MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
            "To: /content/good.pk\n",
            "100% 1.84M/1.84M [00:00<00:00, 184MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
            "To: /content/dev_task_b_labels.csv\n",
            "100% 20.4k/20.4k [00:00<00:00, 37.3MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
            "To: /content/dev_task_c_labels.csv\n",
            "100% 33.5k/33.5k [00:00<00:00, 50.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-8nY2nmkxEOYMSFLCuMUVPnILpFgsOfv&confirm=t\n",
            "To: /content/best_ch.pt\n",
            "100% 5.21G/5.21G [01:02<00:00, 83.5MB/s]\n",
            "Archive:  train_data.zip\n",
            "  inflating: train_all_tasks.csv     \n",
            "replace reddit_1M_unlabelled.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: reddit_1M_unlabelled.csv  \n",
            "  inflating: gab_1M_unlabelled.csv   \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_a.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_b.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_c.zip  \n",
            "Archive:  eval_A.zip\n",
            "  inflating: dev_task_a_entries.csv  \n",
            "Archive:  eval_B.zip\n",
            "  inflating: dev_task_b_entries.csv  \n",
            "Archive:  eval_C.zip\n",
            "  inflating: dev_task_c_entries.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CemXYajcKMwN",
        "outputId": "29f7a7b8-c24c-4085-a9a6-1795db959594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import json\n",
        "import pickle\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import transformers\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from transformers import BertModel, BertTokenizer, DebertaTokenizer, DebertaModel, RobertaTokenizer, RobertaModel, ElectraTokenizer, ElectraModel\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n",
        "# from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import defaultdict, namedtuple, OrderedDict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from itertools import count \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW, Adam, RMSprop\n",
        "from copy import deepcopy\n",
        "from sklearn.utils import shuffle\n",
        "from typing import Union, Callable\n",
        "import random\n",
        "import gdown\n",
        "from torch import Tensor\n",
        "from typing import Optional, Tuple\n",
        "import pickle as pk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "seeds = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
        "seed_idx = 1\n",
        "seed = seeds[seed_idx]\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "deberta = 'microsoft/deberta-v3-large' \n",
        "roberta = 'roberta-large'\n",
        "model_name = deberta\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('train_all_tasks.csv')\n",
        "eval_data_A = pd.read_csv('dev_task_a_entries.csv')\n",
        "eval_data_B = pd.read_csv('dev_task_b_entries.csv')\n",
        "eval_data_C = pd.read_csv('dev_task_c_entries.csv')\n",
        "all_data_edos = pd.read_csv('edos_labelled_aggregated.csv')\n",
        "\n",
        "\n",
        "def tolist(tensor):\n",
        "  return tensor.detach().cpu().tolist()\n",
        "\n",
        "def map_names_2_ids(names):\n",
        "  A = dict()\n",
        "  B = dict()\n",
        "  for id, name in enumerate(names):\n",
        "    A[name] = id\n",
        "    B[id] = name\n",
        "  return A, B\n",
        "\n",
        "def dist(x1, x2):\n",
        "  return (x1 - x2).pow(2).sum(-1).sqrt()\n",
        "\n",
        "def entropy(logits):\n",
        "  probs = F.softmax(logits, dim=-1)\n",
        "  ent = -torch.sum((probs * torch.log2(probs)),dim=1)\n",
        "  return ent\n",
        "\n",
        "train_data = train_data[train_data['label_sexist'] == 'sexist'].reset_index(drop=True)\n",
        "test_data = deepcopy(all_data_edos[(all_data_edos['split'] == 'test') & (all_data_edos['label_sexist'] =='sexist')]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "label_category_raw = np.unique(train_data['label_category']).tolist()\n",
        "label_category_map, category_label_map = map_names_2_ids(label_category_raw)\n",
        "train_data['Tag_B'] = [label_category_map[i[1]] for i in train_data['label_category'].iteritems()]\n",
        "label_category = list(label_category_map.keys())\n",
        "label_category = list(map(lambda x: re.sub('^\\d+\\.\\d*', '', x).strip(), label_category))\n",
        "eval_label_B = pd.read_csv('dev_task_b_labels.csv')\n",
        "eval_B = eval_data_B.merge(eval_label_B, on='rewire_id')\n",
        "eval_B['Tag_B'] = [label_category_map[i[1]] for i in eval_B['label'].iteritems()]\n",
        "num_labels_B = len(train_data['label_category'].unique())\n",
        "\n",
        "label_vector_raw = np.unique(train_data['label_vector']).tolist()\n",
        "label_vector_map, vector_label_map = map_names_2_ids(label_vector_raw)\n",
        "train_data['Tag_C'] = [label_vector_map[i[1]] for i in train_data['label_vector'].iteritems()]\n",
        "label_vector = list(label_vector_map.keys())\n",
        "label_vector = list(map(lambda x: re.sub('^\\d+\\.\\d*', '', x).strip(), label_vector))\n",
        "eval_label_C = pd.read_csv('dev_task_c_labels.csv')\n",
        "eval_C = eval_data_C.merge(eval_label_C, on='rewire_id')\n",
        "eval_C['Tag_C'] = [label_vector_map[i[1]] for i in eval_C['label'].iteritems()] \n",
        "test_data['Tag_C'] = [label_vector_map[i[1]] for i in test_data['label_vector'].iteritems()] \n",
        "num_labels_C = len(train_data['label_vector'].unique())\n",
        "\n",
        "train_dataframe = train_data\n",
        "eval_dataframe = eval_C\n",
        "test_dataframe = test_data\n",
        "new_l = label_category\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array(list(range(num_labels_C))), y=train_data['Tag_C'].values.tolist()).tolist()\n",
        "\n",
        "\n",
        "class SexistDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, max_length=100, is_test=False):\n",
        "    self.dataframe = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    self.labels_names = f'{tokenizer.sep_token}'.join(new_l)\n",
        "    self.is_test = is_test\n",
        "\n",
        "    self.labels_tokens = []\n",
        "    for label_name in new_l:\n",
        "      label_tokens = tokenizer(label_name, add_special_tokens=False)\n",
        "      self.labels_tokens.append(label_tokens['input_ids'])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.dataframe.loc[idx]\n",
        "    tokenized_text = tokenizer(\n",
        "          sample['text'],\n",
        "          max_length=self.max_length,\n",
        "          padding='max_length',\n",
        "          truncation='only_first',\n",
        "          return_tensors='pt')\n",
        "\n",
        "    # find the first token of labels\n",
        "    input_ids = tokenized_text['input_ids']\n",
        "    labels_start = (input_ids == tokenizer.sep_token_id).nonzero().contiguous().view(-1).tolist()[1] + 2\n",
        "\n",
        "    labels_tokens_span = []\n",
        "    c_token = labels_start\n",
        "    # print(labels_start)\n",
        "    for label_tokens in self.labels_tokens:\n",
        "\n",
        "      labels_tokens_span.append([c_token, c_token + len(label_tokens) - 1])\n",
        "      c_token += len(label_tokens) + 1\n",
        "    tokenized_text['labels_tokens_span'] = torch.tensor(labels_tokens_span)\n",
        "    if not self.is_test:\n",
        "      labels_C = torch.LongTensor([sample['Tag_C']])\n",
        "      tokenized_text['Tag_C'] = labels_C\n",
        "    return tokenized_text\n",
        "\n",
        "\n",
        "class UnlabeledDataset:\n",
        "  def __init__(self, dataframe, tokenizer, max_length=70, batch_size=10):\n",
        "    self.data = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    self.batch_size = batch_size\n",
        "    self.idxs = np.random.permutation(range(len(self.data)))\n",
        "    self.current_idx = 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def combine_tenors(self, tensors_list):\n",
        "    combined_tensor = dict()\n",
        "    keys = tensors_list[0].keys()\n",
        "    for key in keys:\n",
        "      combined_tensor[key] = torch.cat([tensor[key] for tensor in tensors_list])\n",
        "    return combined_tensor\n",
        "\n",
        "  def next(self):\n",
        "    if self.current_idx >= len(self.data) - 1:\n",
        "      self.current_idx = 0\n",
        "    compact_data = self.data.iloc[self.idxs[self.current_idx: self.current_idx + self.batch_size]]\n",
        "    self.current_idx += self.batch_size\n",
        "    all_t = [self.tokenize(data) for _, data in compact_data.iterrows()]\n",
        "    T = self.combine_tenors(all_t)\n",
        "    return T\n",
        "\n",
        "  def tokenize(self, data):\n",
        "    return self.tokenizer(data['text'], padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.sqrt_dim = np.sqrt(dim)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
        "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
        "\n",
        "        if mask is not None:\n",
        "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
        "\n",
        "        attn = F.softmax(score, -1)\n",
        "        context = torch.bmm(attn, value)\n",
        "        return context, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, num_heads: int = 8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
        "\n",
        "        self.d_head = int(d_model / num_heads)\n",
        "        self.num_heads = num_heads\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
        "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
        "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
        "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: Tensor,\n",
        "            key: Tensor,\n",
        "            value: Tensor,\n",
        "            mask: Optional[Tensor] = None\n",
        "    ) -> Tuple[Tensor, Tensor]:\n",
        "        batch_size = value.size(0)\n",
        "\n",
        "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n",
        "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n",
        "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n",
        "\n",
        "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n",
        "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n",
        "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n",
        "\n",
        "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
        "\n",
        "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
        "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n",
        "\n",
        "        return context, attn\n",
        "\n",
        "def exists(value):\n",
        "    return value is not None\n",
        "\n",
        "\n",
        "def default(value, default):\n",
        "    if exists(value):\n",
        "        return value\n",
        "    return default\n",
        "\n",
        "\n",
        "def inf_norm(x):\n",
        "    return torch.norm(x, p=float(\"inf\"), dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "def kl_loss(input, target, reduction=\"batchmean\"):\n",
        "    return F.kl_div(\n",
        "        F.log_softmax(input, dim=-1),\n",
        "        F.softmax(target, dim=-1),\n",
        "        reduction=reduction,\n",
        "    )\n",
        "\n",
        "\n",
        "def sym_kl_loss(input, target, reduction=\"batchmean\", alpha=1.0):\n",
        "    return alpha * F.kl_div(\n",
        "        F.log_softmax(input, dim=-1),\n",
        "        F.softmax(target.detach(), dim=-1),\n",
        "        reduction=reduction,\n",
        "    ) + F.kl_div(\n",
        "        F.log_softmax(target, dim=-1),\n",
        "        F.softmax(input.detach(), dim=-1),\n",
        "        reduction=reduction,\n",
        "    )\n",
        "\n",
        "\n",
        "def js_loss(input, target, reduction=\"batchmean\", alpha=1.0):\n",
        "    mean_proba = 0.5 * (\n",
        "        F.softmax(input.detach(), dim=-1) + F.softmax(target.detach(), dim=-1)\n",
        "    )\n",
        "    return alpha * (\n",
        "        F.kl_div(F.log_softmax(input, dim=-1), mean_proba, reduction=reduction)\n",
        "        + F.kl_div(F.log_softmax(target, dim=-1), mean_proba, reduction=reduction)\n",
        "    )\n",
        "\n",
        "\n",
        "def ntxent(logits, labels, temp=.07):\n",
        "  def ntx_loss(a, p, n, temp=temp):\n",
        "    a = a.unsqueeze(0) if a.dim() == 1 else a\n",
        "    p = p.unsqueeze(0) if p.dim() == 1 else p\n",
        "    n = n.unsqueeze(0) if n.dim() == 1 else n\n",
        "    assert a.dim() == 2\n",
        "    assert p.dim() == 2\n",
        "    assert n.dim() == 2\n",
        "    a_p = a\n",
        "    a_n = a.repeat(n.shape[0], 1)\n",
        "    p_sim = F.cosine_similarity(a_p, p, dim=-1) / temp\n",
        "    n_sim = F.cosine_similarity(a_n, n, dim=-1) / temp\n",
        "\n",
        "    # apply numeric stability\n",
        "    max_val = torch.max(n_sim).detach()\n",
        "    numerator = torch.exp(p_sim - max_val)\n",
        "    denominator = torch.exp(n_sim - max_val).sum()\n",
        "    loss = -torch.log(numerator / (denominator + numerator) + 1e-6)\n",
        "    if loss.isnan():\n",
        "      print(numerator, denominator)\n",
        "      print(p_sim)\n",
        "      print(len(n))\n",
        "    # print(loss)\n",
        "    return loss.mean()\n",
        "\n",
        "  def dist(x1, x2):\n",
        "    return (x1 - x2).pow(2).sum(-1).sqrt()\n",
        "\n",
        "  con_losses = list()\n",
        "  for i, (logit, label) in enumerate(zip(logits, labels)):\n",
        "    ps = (labels == label)\n",
        "    ns = (labels != label).nonzero().view(-1)\n",
        "    ps[i] = False\n",
        "    ps = ps.nonzero().view(-1)\n",
        "    if len(ns):\n",
        "      for p in ps:\n",
        "        a_logit = logits[i]\n",
        "        p_logit = logits[p]\n",
        "        ns_logit = logits[ns]\n",
        "        A = ntx_loss(a_logit, p_logit, ns_logit)\n",
        "        con_losses.append(A)\n",
        "\n",
        "  if len(con_losses) > 0:\n",
        "    all_con_loss = torch.stack(con_losses).mean()\n",
        "  else:\n",
        "    all_con_loss = torch.tensor(0.)\n",
        "  return all_con_loss\n",
        "\n",
        "\n",
        "class SMARTLoss(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_fn: Callable,\n",
        "        loss_last_fn: Callable = None, \n",
        "        norm_fn: Callable = inf_norm, \n",
        "        num_steps: int = 1,\n",
        "        step_size: float = 1e-3, \n",
        "        epsilon: float = 1e-6,\n",
        "        noise_var: float = 1e-5\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.model = model \n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_last_fn = default(loss_last_fn, loss_fn)\n",
        "        self.norm_fn = norm_fn\n",
        "        self.num_steps = num_steps \n",
        "        self.step_size = step_size\n",
        "        self.epsilon = epsilon \n",
        "        self.noise_var = noise_var\n",
        "     \n",
        "    @torch.enable_grad()   \n",
        "    def forward(self, embed, state, batch_labels_tokens_span, attention_mask):\n",
        "        noise = torch.randn_like(embed, requires_grad = True) * self.noise_var \n",
        "        \n",
        "        # Indefinite loop with counter \n",
        "        for i in count():\n",
        "            # Compute perturbed embed and states \n",
        "            embed_perturbed = embed + noise \n",
        "            state_perturbed, _ = self.model(embed_perturbed, batch_labels_tokens_span, True, attention_mask) \n",
        "            # Return final loss if last step (undetached state)\n",
        "            if i == self.num_steps: \n",
        "                return self.loss_last_fn(state_perturbed, state) \n",
        "            # Compute perturbation loss (detached state)\n",
        "            loss = self.loss_fn(state_perturbed, state.detach())\n",
        "            # Compute noise gradient ∂loss/∂noise\n",
        "            noise_gradient, = torch.autograd.grad(loss, noise)\n",
        "            # Move noise towards gradient to change state as much as possible \n",
        "            step = noise + self.step_size * noise_gradient \n",
        "            # Normalize new noise step into norm induced ball \n",
        "            step_norm = self.norm_fn(step)\n",
        "            noise = step / (step_norm + self.epsilon)\n",
        "            # Reset noise gradients for next step\n",
        "            noise = noise.detach().requires_grad_()\n",
        "\n",
        "class PGD():\n",
        "\n",
        "    def __init__(self, model,emb_name,epsilon=1.,alpha=0.3):\n",
        "        # The emb_name parameter should be replaced with the parameter name of the embedding in your model\n",
        "        self.model = model\n",
        "        self.emb_name = emb_name\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.emb_backup = {}\n",
        "        self.grad_backup = {}\n",
        "\n",
        "    # adversarial training : attack to change embedding abit with regards projected gradiant descent\n",
        "    def attack(self,first_strike=False):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and self.emb_name in name:\n",
        "                if first_strike:\n",
        "                    # print('tt', param.data)\n",
        "                    self.emb_backup[name] = param.data.clone()\n",
        "                norm = torch.norm(param.grad)\n",
        "                if norm != 0:\n",
        "                    # Compute new params\n",
        "                    r_at = self.alpha * param.grad / norm\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = self.project(name, param.data, self.epsilon)\n",
        "\n",
        "    # Restore to the back-up embeddings\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and self.emb_name in name:\n",
        "                assert name in self.emb_backup\n",
        "                param.data = self.emb_backup[name]\n",
        "        self.emb_backup = {}\n",
        "\n",
        "    # Project Gradiant Descent\n",
        "    def project(self, param_name, param_data, epsilon):\n",
        "        r = param_data - self.emb_backup[param_name]\n",
        "        if torch.norm(r) > epsilon:\n",
        "            r = epsilon * r / torch.norm(r)\n",
        "        return self.emb_backup[param_name] + r\n",
        "\n",
        "    # Back-up parameters\n",
        "    def backup_grad(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and 'pooler' not in name:\n",
        "                self.grad_backup[name] = param.grad.clone()\n",
        "\n",
        "    # Restore grad parameters\n",
        "    def restore_grad(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and 'pooler' not in name:\n",
        "                param.grad = self.grad_backup[name]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SexistModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.transformer = model\n",
        "        hidden_size = self.transformer.config.hidden_size\n",
        "        self.dropout = nn.Dropout(p=.3)\n",
        "        self.head = nn.Linear(hidden_size, num_labels_C)\n",
        "\n",
        "\n",
        "    def integrate(self, batch_output, batch_labels_tokens_span):\n",
        "      batch_size = batch_output.shape[0]\n",
        "      integrated_batch = []\n",
        "      for i in range(batch_size):\n",
        "        integrated_sample_labels = []\n",
        "        output = batch_output[i]\n",
        "        labels_tokens_span = batch_labels_tokens_span[i]\n",
        "        for label_tokens_span in labels_tokens_span:\n",
        "          integrated_label = output[label_tokens_span[0].item(): label_tokens_span[1].item() + 1].mean(0).view(-1)\n",
        "          assert integrated_label.shape[0] == self.transformer.config.hidden_size\n",
        "          integrated_sample_labels.append(integrated_label)\n",
        "        integrated_sample_labels = torch.stack(integrated_sample_labels)\n",
        "        integrated_batch.append(integrated_sample_labels)\n",
        "      integrated_batch = torch.stack(integrated_batch)\n",
        "      return integrated_batch\n",
        "\n",
        "    def forward(self, x, batch_labels_tokens_span, vat=False, attention_mask=None):\n",
        "        if vat:\n",
        "          hidden = self.transformer(inputs_embeds=x, attention_mask=attention_mask).last_hidden_state\n",
        "        else:\n",
        "          hidden = self.transformer(**x).last_hidden_state\n",
        "        cls = hidden[:, 0, :]\n",
        "        # label_vecs = self.integrate(hidden, batch_labels_tokens_span)\n",
        "        # hidden = self.att_layer(cls.unsqueeze(1), label_vecs, label_vecs)[0]\n",
        "        # hidden = self.dropout(hidden)\n",
        "        x = self.head(cls)\n",
        "        x = x.view(-1, num_labels_C)\n",
        "        return x, hidden[:, 0, :]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(dataloader, model, device, loss_fn, optimizer, scheduler, stage, ul_dataset, use_contrastive=False,\n",
        "          use_adv=True, use_vadv=False, use_ul=False, vat_weight=.5, ul_weight=.5, con_weight=.5, adv_use_every_layer=True):\n",
        "  \n",
        "  model.train()\n",
        "  named_weights = [n for n, _ in model.named_parameters() if 'dense.weight' in n and 'pooler' not in n] + [\"word_embeddings.\"]\n",
        "  loss_collection = [[], [], [], [], []]\n",
        "  for step, data in enumerate(dataloader):\n",
        "\n",
        "    if adv_use_every_layer:\n",
        "      rand_layer = random.sample(named_weights, 1)[0] \n",
        "      adv_layer = rand_layer\n",
        "    else:\n",
        "      adv_rand = random.uniform(0, 1) \n",
        "      if adv_rand > .7:\n",
        "        adv_layer = \"word_embeddings.\"\n",
        "      else:\n",
        "        rand_layer = random.sample(named_weights, 1)[0] \n",
        "        adv_layer = rand_layer\n",
        "    pgd = PGD(\n",
        "      model=model,\n",
        "      emb_name=adv_layer\n",
        "    )\n",
        "\n",
        "\n",
        "    c_batch_size = data['input_ids'].shape[0]\n",
        "    labels = data.pop('Tag_C').to(device).view(-1)\n",
        "    for key in data:\n",
        "      data[key] = data[key].to(device).view(c_batch_size, -1)\n",
        "    batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n",
        "\n",
        "    logits, _ = model(data, batch_labels_tokens_span)\n",
        "\n",
        "    ce_loss = loss_fn(logits, labels)\n",
        "    ce_loss.backward()\n",
        "    loss_collection[0].append(ce_loss.item())\n",
        "\n",
        "\n",
        "    \n",
        "    if use_adv:\n",
        "      # PGD Start\n",
        "        pgd.backup_grad()\n",
        "        attack_times = 4\n",
        "        for attack_time in range(attack_times):\n",
        "            # Add adversarial perturbation to the embedding, backup param.data during the first attack\n",
        "            pgd.attack(first_strike=(attack_time==0))\n",
        "            if attack_time != attack_times-1:\n",
        "              model.zero_grad()\n",
        "            else:\n",
        "              pgd.restore_grad()\n",
        "            logits_adv, _ = model(data, batch_labels_tokens_span)\n",
        "            loss_adv = loss_fn(logits_adv, labels)\n",
        "            loss_collection[1].append(loss_adv.item())\n",
        "            loss_adv.backward()\n",
        "        # Restore embedding parameters\n",
        "        pgd.restore() \n",
        "\n",
        "    if use_contrastive:\n",
        "      embeddings = model.get_embeddings(data['input_ids'].to(device))\n",
        "      model.set_attention_mask(data['attention_mask'].to(device))\n",
        "      logits = model(embeddings, batch_labels_tokens_span)\n",
        "      con_loss = ntxent(logits, labels) * con_weight\n",
        "      if con_loss.requires_grad:\n",
        "        con_loss.backward()\n",
        "      loss_collection[2].append(con_loss.item())\n",
        "\n",
        "\n",
        "    if use_vadv:\n",
        "      vat_loss_fn = SMARTLoss(model = model, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n",
        "      # Compute VAT loss\n",
        "      embeddings = model.transformer.embeddings(data['input_ids'], data['token_type_ids'])\n",
        "      logits, _ = model(embeddings, batch_labels_tokens_span, True, data['attention_mask'])\n",
        "      vat_loss = vat_loss_fn(embeddings, logits, batch_labels_tokens_span, data['attention_mask']) \n",
        "      # Merge losses \n",
        "      vat_loss = vat_weight * vat_loss\n",
        "      vat_loss.backward()\n",
        "      loss_collection[3].append(vat_loss.item())    \n",
        "\n",
        "\n",
        "    if use_ul:\n",
        "      ul_data = ul_dataset.next()\n",
        "      c_batch_size = ul_data['input_ids'].shape[0]\n",
        "      for key in ul_data:\n",
        "        ul_data[key] = ul_data[key].to(device).view(c_batch_size, -1)\n",
        "\n",
        "      ul_embeddings = model.get_embeddings(ul_data['input_ids'].to(device))\n",
        "      model.set_attention_mask(ul_data['attention_mask'].to(device))\n",
        "      ul_logits = model(ul_embeddings)\n",
        "\n",
        "      vat_loss_fn = SMARTLoss(model = model, loss_fn = kl_loss, loss_last_fn = sym_kl_loss)\n",
        "      vat_loss = vat_loss_fn(ul_embeddings, ul_logits) \n",
        "      ul_loss = ul_weight * vat_loss\n",
        "      loss_collection[4].append(ul_loss.item())\n",
        "      ul_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    scheduler.step()\n",
        "    \n",
        "    if len(loss_collection[0]) % log_step == 0:\n",
        "      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CE Loss {round(sum(loss_collection[0]) / (len(loss_collection[0]) + 1e-8), 4)}')\n",
        "      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | ADV Loss {round(sum(loss_collection[1]) / (len(loss_collection[1]) + 1e-8), 4)}')\n",
        "      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | CON Loss {round(sum(loss_collection[2]) / (len(loss_collection[2]) + 1e-8), 4)}')\n",
        "      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | VAT Loss {round(sum(loss_collection[3]) / (len(loss_collection[3]) + 1e-8), 4)}')\n",
        "      print(f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step + 1}/{len(train_dataloader)}] | UL Loss {round(sum(loss_collection[4]) / (len(loss_collection[4]) + 1e-8), 4)}')\n",
        "      print('------------------------------------------------')\n",
        "      loss_collection = [[] for _ in range(5)]\n",
        "\n",
        "\n",
        "def eval(dataloader, model, device):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    all_preds = list()\n",
        "    all_hidden = list()\n",
        "    for data in dataloader:\n",
        "      c_batch_size = data['input_ids'].shape[0]\n",
        "      for key in data:\n",
        "        data[key] = data[key].to(device).view(c_batch_size, -1)\n",
        "      batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n",
        "      Tag_B = data.pop('Tag_C').to(device).view(-1)\n",
        "      logits, hiddens = model(data, batch_labels_tokens_span)\n",
        "      all_hidden.extend(tolist(hiddens))\n",
        "      preds = tolist(logits.argmax(1).view(-1))\n",
        "      all_preds.extend(preds)\n",
        "  return all_preds, all_hidden\n",
        "\n",
        "\n",
        "def test(dataloader, model, device):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    all_preds = list()\n",
        "\n",
        "    for data in dataloader:\n",
        "      c_batch_size = data['input_ids'].shape[0]\n",
        "      for key in data:\n",
        "        data[key] = data[key].to(device).view(c_batch_size, -1)\n",
        "      batch_labels_tokens_span = data.pop('labels_tokens_span').view(-1, num_labels_B, 2)\n",
        "      logits, _ = model(data, batch_labels_tokens_span)\n",
        "      preds = tolist(logits.argmax(1).view(-1))\n",
        "      all_preds.extend(preds)\n",
        "  return all_preds\n",
        "\n",
        "epochs = 30\n",
        "lr = 6e-6\n",
        "beta_1 = .9\n",
        "beta_2 = .999\n",
        "eps = 1e-6\n",
        "log_step = 100\n",
        "batch_size = 10\n",
        "weight_decay = 7e-3\n",
        "max_length = 70\n",
        "loss_file = 'loss.txt'\n",
        "eval_file = 'eval.txt'\n",
        "\n",
        "vat_weight = .5\n",
        "ul_weight = .5\n",
        "ent_weight = .5\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "# sexist_model = ExtractedRoBERTa(deepcopy(model)).to(device)\n",
        "sexist_model = SexistModel(deepcopy(model)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device)).to(device)\n",
        "loss_collection = []\n",
        "\n",
        "train_dataset = SexistDataset(train_dataframe, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "eval_dataset = SexistDataset(eval_dataframe, tokenizer, max_length)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset = SexistDataset(test_dataframe, tokenizer, max_length)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "opt_step = 0\n",
        "optimization_steps = epochs * len(train_dataloader)\n",
        "warmup_ratio = .0\n",
        "warmup_steps = int(optimization_steps * warmup_ratio)\n",
        "\n",
        "\n",
        "optimizer = AdamW(sexist_model.parameters(), lr=lr, betas=(beta_1,beta_2), eps=eps, weight_decay=weight_decay)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps, \n",
        "    num_training_steps=optimization_steps)\n",
        "\n",
        "best_f1 = 0.\n",
        "best_model = None\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = '.'\n",
        "filename = os.path.join(checkpoint_dir, 'best_ch.pt')\n",
        "\n",
        "\n",
        "def save_model(epoch, model, optimizer, scheduler):\n",
        "  filename = os.path.join(checkpoint_dir, 'best_ch.pt')\n",
        "  torch.save(\n",
        "      {'epoch': epoch,\n",
        "       'model_state_dict': model.state_dict(),\n",
        "       'optimizer_state_dict': optimizer.state_dict(),\n",
        "       'scheduler_state_dict': scheduler.state_dict()}, \n",
        "        filename)\n",
        "\n",
        "def load_model():\n",
        "  if os.path.exists(filename):\n",
        "    saved_dict = torch.load(filename)\n",
        "    return True, saved_dict\n",
        "  else:\n",
        "    return False, None\n",
        "    \n",
        "\n",
        "def early_stop(scores, current_score, patience):\n",
        "  if len(scores) < patience:\n",
        "    return False\n",
        "  else:\n",
        "    for score in scores[-patience: ]:\n",
        "      if current_score > score:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "all_f1 = list()\n",
        "patience = 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, saved_dict = load_model()\n",
        "sexist_model.load_state_dict(saved_dict['model_state_dict'])\n",
        "preds_C_test, _ = eval(test_dataloader, sexist_model, device)\n",
        "f1_macro_C_test = f1_score(test_data['Tag_C'].values.tolist(), preds_C_test, average='macro')\n",
        "pr_C_test = precision_score(test_data['Tag_C'].values.tolist(), preds_C_test, average='macro')\n",
        "rc_C_test = recall_score(test_data['Tag_C'].values.tolist(), preds_C_test, average='macro')"
      ],
      "metadata": {
        "id": "iabx6G9XKtaK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_macro_C_test, pr_C_test, rc_C_test"
      ],
      "metadata": {
        "id": "5oplaYegz4lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367c8f8b-22fe-4d68-b643-5fe2131a6156"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5251915699955355, 0.5367206261964147, 0.5289212136458749)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}
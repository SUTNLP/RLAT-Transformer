{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L5Y2w1BFt0t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46549c8a-360c-42d5-96ad-b346922cfbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-26 10:55:42--  https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3846925 (3.7M) [text/plain]\n",
            "Saving to: ‘edos_labelled_aggregated.csv’\n",
            "\n",
            "edos_labelled_aggre 100%[===================>]   3.67M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-26 10:55:43 (189 MB/s) - ‘edos_labelled_aggregated.csv’ saved [3846925/3846925]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.22.4)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (23.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (6.0.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.26.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=3bf623e427aa0fe86fdaec2d00e2cb709017176f9537debde4556b1de31f3ccb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\n",
            "To: /content/train_data.zip\n",
            "100% 87.1M/87.1M [00:00<00:00, 145MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
            "To: /content/eval_A.zip\n",
            "100% 125k/125k [00:00<00:00, 97.4MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
            "To: /content/eval_B.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 47.2MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
            "To: /content/eval_C.zip\n",
            "100% 32.6k/32.6k [00:00<00:00, 40.7MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
            "To: /content/gab_1M_unlabelled.csv\n",
            "100% 95.9M/95.9M [00:02<00:00, 44.4MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
            "To: /content/reddit_1M_unlabelled.csv\n",
            "100% 98.0M/98.0M [00:01<00:00, 51.3MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
            "To: /content/good.pk\n",
            "100% 1.84M/1.84M [00:00<00:00, 176MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
            "To: /content/dev_task_b_labels.csv\n",
            "100% 20.4k/20.4k [00:00<00:00, 31.0MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
            "To: /content/dev_task_c_labels.csv\n",
            "100% 33.5k/33.5k [00:00<00:00, 49.8MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EKFujUoPzVspdwoNSFsLvw9HBOUMZ1ys\n",
            "To: /content/test_task_b_entries.csv\n",
            "100% 157k/157k [00:00<00:00, 99.7MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oDHszlQtjfYOXpe6_v3zdptm5al2FQkl\n",
            "To: /content/test_task_c_entries.csv\n",
            "100% 157k/157k [00:00<00:00, 103MB/s]\n",
            "Archive:  train_data.zip\n",
            "  inflating: train_all_tasks.csv     \n",
            "replace reddit_1M_unlabelled.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: reddit_1M_unlabelled.csv  \n",
            "  inflating: gab_1M_unlabelled.csv   \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_a.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_b.zip  \n",
            " extracting: EXAMPLE_SUBMISSION_dev_task_c.zip  \n",
            "Archive:  eval_A.zip\n",
            "  inflating: dev_task_a_entries.csv  \n",
            "Archive:  eval_B.zip\n",
            "  inflating: dev_task_b_entries.csv  \n",
            "Archive:  eval_C.zip\n",
            "  inflating: dev_task_c_entries.csv  \n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/rewire-online/edos/main/data/edos_labelled_aggregated.csv\n",
        "! pip install transformers\n",
        "! pip install optuna\n",
        "! pip install sentencepiece\n",
        "! pip install gdown\n",
        "! pip install sentence-transformers\n",
        "\n",
        "! gdown \"1UjEYJOauAanJ0UzuSRrCVLn6GHvJyWus&confirm=t\"\n",
        "! gdown --id 16YwJWsljY2r2R4Fzh4xLilSbPjowlZr_\n",
        "! gdown --id 1YDr6ejvJPiQL3HAWR7i71GanzWsfxlQo\n",
        "! gdown --id 11hYRWLmxyWd3j-rQsKTPS1fCuQvSFWcl\n",
        "! gdown --id 1z2FvuIDZPUTKXJ0_Z6VBvFEzRteTLIa5\n",
        "! gdown --id 1hEX89Ffv_lu8uIoOuX5trX0RqYFiI0Of\n",
        "! gdown --id 1MD4pEghAgkpkxhr9Tyx_cEqGggki_XBu\n",
        "! gdown --id 1zCW194SsKQ6U3sUEJ1eB7PiIKcOfdki0\n",
        "! gdown --id 1yh77Xp7mwjjaYrPX6Y8OQchXSSqYIso-\n",
        "\n",
        "! gdown --id 1EKFujUoPzVspdwoNSFsLvw9HBOUMZ1ys\n",
        "! gdown --id 1oDHszlQtjfYOXpe6_v3zdptm5al2FQkl\n",
        "\n",
        "! yes A | unzip train_data.zip\n",
        "! unzip eval_A.zip\n",
        "! unzip eval_B.zip\n",
        "! unzip eval_C.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python Task_10_C.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcH_C6nt2je",
        "outputId": "9eeccc18-9eaf-4e5f-aa6d-94db2fa0cb3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-26 10:56:56.719852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-26 10:56:57.905011: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 10:56:57.905128: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 10:56:57.905147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 7.79kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 580/580 [00:00<00:00, 94.6kB/s]\n",
            "Downloading (…)\"spm.model\";: 100% 2.46M/2.46M [00:00<00:00, 71.3MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 874M/874M [00:11<00:00, 73.8MB/s]\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "EPOCH [1/30] | STEP [100/340] | CE Loss 2.5123\n",
            "EPOCH [1/30] | STEP [100/340] | ADV Loss 2.5561\n",
            "EPOCH [1/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | STEP [200/340] | CE Loss 2.4269\n",
            "EPOCH [1/30] | STEP [200/340] | ADV Loss 2.5063\n",
            "EPOCH [1/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | STEP [300/340] | CE Loss 2.3801\n",
            "EPOCH [1/30] | STEP [300/340] | ADV Loss 2.4354\n",
            "EPOCH [1/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [1/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [1/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [1/30] | Current F1-Macro 6.89\n",
            "EPOCH [1/30] | Best F1-Macro 6.89\n",
            "[[ 0  0  0  0  0  2  0  0  0  0  6]\n",
            " [ 0  0  0  5  0  6  0  0  0  0 25]\n",
            " [ 0  0  0  3  0  5  0  0  0  0 94]\n",
            " [ 0  0  0  9  0 33  0  0  0  0 54]\n",
            " [ 0  0  0  1  0  4  0  0  0  0 24]\n",
            " [ 0  0  0  6  0 39  0  0  0  0 46]\n",
            " [ 0  0  0  0  0  2  0  0  0  0 58]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  7]\n",
            " [ 0  0  0  0  0  1  0  0  0  0 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 37]]\n",
            "not early stopping\n",
            "EPOCH [2/30] | STEP [100/340] | CE Loss 2.3258\n",
            "EPOCH [2/30] | STEP [100/340] | ADV Loss 2.4222\n",
            "EPOCH [2/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | STEP [200/340] | CE Loss 2.2714\n",
            "EPOCH [2/30] | STEP [200/340] | ADV Loss 2.3635\n",
            "EPOCH [2/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | STEP [300/340] | CE Loss 2.1914\n",
            "EPOCH [2/30] | STEP [300/340] | ADV Loss 2.3084\n",
            "EPOCH [2/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [2/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [2/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [2/30] | Current F1-Macro 18.18\n",
            "EPOCH [2/30] | Best F1-Macro 18.18\n",
            "[[ 0  5  0  0  0  2  1  0  0  0  0]\n",
            " [ 0 14  0  0  0 13  9  0  0  0  0]\n",
            " [ 0  5  8  2  0 13 71  0  0  0  3]\n",
            " [ 0  0  4 29  0 52 11  0  0  0  0]\n",
            " [ 0  5  1  1  0  9 13  0  0  0  0]\n",
            " [ 0  3  1 21  0 62  4  0  0  0  0]\n",
            " [ 0  1  0  0  0  6 49  0  0  0  4]\n",
            " [ 0  0  0  0  0  1  8  0  0  0  0]\n",
            " [ 0  1  1  0  0  1  4  0  0  0  0]\n",
            " [ 0  4  0  0  0  2  5  0  0  0  0]\n",
            " [ 0  1  0  0  0  3 27  0  0  0  6]]\n",
            "not early stopping\n",
            "EPOCH [3/30] | STEP [100/340] | CE Loss 2.0671\n",
            "EPOCH [3/30] | STEP [100/340] | ADV Loss 2.2619\n",
            "EPOCH [3/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | STEP [200/340] | CE Loss 1.9927\n",
            "EPOCH [3/30] | STEP [200/340] | ADV Loss 2.2154\n",
            "EPOCH [3/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | STEP [300/340] | CE Loss 1.9789\n",
            "EPOCH [3/30] | STEP [300/340] | ADV Loss 2.2121\n",
            "EPOCH [3/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [3/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [3/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [3/30] | Current F1-Macro 24.96\n",
            "EPOCH [3/30] | Best F1-Macro 24.96\n",
            "[[ 0  6  0  0  0  1  1  0  0  0  0]\n",
            " [ 0 20  4  0  0  7  5  0  0  0  0]\n",
            " [ 0  0 15  3  6  7 66  0  0  0  5]\n",
            " [ 0  0  4 41  7 34 10  0  0  0  0]\n",
            " [ 0  2  5  1  1  9 11  0  0  0  0]\n",
            " [ 0  0  1 24  1 63  2  0  0  0  0]\n",
            " [ 0  1  1  0  1  4 49  0  0  0  4]\n",
            " [ 0  0  2  0  0  1  6  0  0  0  0]\n",
            " [ 0  0  0  0  1  1  5  0  0  0  0]\n",
            " [ 0  1  1  0  1  1  7  0  0  0  0]\n",
            " [ 0  1  1  0  1  2 20  0  0  0 12]]\n",
            "not early stopping\n",
            "EPOCH [4/30] | STEP [100/340] | CE Loss 1.8264\n",
            "EPOCH [4/30] | STEP [100/340] | ADV Loss 2.0616\n",
            "EPOCH [4/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | STEP [200/340] | CE Loss 1.7404\n",
            "EPOCH [4/30] | STEP [200/340] | ADV Loss 2.0905\n",
            "EPOCH [4/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | STEP [300/340] | CE Loss 1.7046\n",
            "EPOCH [4/30] | STEP [300/340] | ADV Loss 2.0053\n",
            "EPOCH [4/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [4/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [4/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [4/30] | Current F1-Macro 34.35\n",
            "EPOCH [4/30] | Best F1-Macro 34.35\n",
            "[[ 0  7  0  0  0  1  0  0  0  0  0]\n",
            " [ 0 33  1  0  1  0  1  0  0  0  0]\n",
            " [ 0  3 46  7 26  0 14  0  0  0  6]\n",
            " [ 0  4  6 57 15 13  0  0  0  1  0]\n",
            " [ 0  5  3  2 16  1  2  0  0  0  0]\n",
            " [ 0  3  1 33  8 46  0  0  0  0  0]\n",
            " [ 0  3 20  0  6  1 26  0  0  0  4]\n",
            " [ 0  0  3  0  2  1  2  0  0  0  1]\n",
            " [ 0  0  2  0  1  1  2  0  0  0  1]\n",
            " [ 0  2  1  0  2  0  1  0  0  1  4]\n",
            " [ 0  1  4  2  2  0  7  0  0  1 20]]\n",
            "not early stopping\n",
            "EPOCH [5/30] | STEP [100/340] | CE Loss 1.446\n",
            "EPOCH [5/30] | STEP [100/340] | ADV Loss 1.8626\n",
            "EPOCH [5/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | STEP [200/340] | CE Loss 1.407\n",
            "EPOCH [5/30] | STEP [200/340] | ADV Loss 1.8113\n",
            "EPOCH [5/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | STEP [300/340] | CE Loss 1.4022\n",
            "EPOCH [5/30] | STEP [300/340] | ADV Loss 1.8301\n",
            "EPOCH [5/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [5/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [5/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [5/30] | Current F1-Macro 50.54\n",
            "EPOCH [5/30] | Best F1-Macro 50.54\n",
            "[[ 5  2  0  0  0  1  0  0  0  0  0]\n",
            " [ 0 30  2  0  0  1  1  0  0  1  1]\n",
            " [ 0  0 53  5  2  1 27  0  0  8  6]\n",
            " [ 0  4 10 52  5 17  4  0  0  4  0]\n",
            " [ 0  5  7  2  9  2  4  0  0  0  0]\n",
            " [ 0  0  5 22  2 59  0  1  0  2  0]\n",
            " [ 0  1  6  0  2  1 39  1  0  4  6]\n",
            " [ 0  0  2  0  0  1  2  2  0  0  2]\n",
            " [ 0  0  0  1  1  0  4  0  0  1  0]\n",
            " [ 0  1  0  0  0  0  1  0  0  8  1]\n",
            " [ 0  1  2  1  0  1  4  0  0  3 25]]\n",
            "not early stopping\n",
            "EPOCH [6/30] | STEP [100/340] | CE Loss 1.1044\n",
            "EPOCH [6/30] | STEP [100/340] | ADV Loss 1.5518\n",
            "EPOCH [6/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | STEP [200/340] | CE Loss 1.0632\n",
            "EPOCH [6/30] | STEP [200/340] | ADV Loss 1.6157\n",
            "EPOCH [6/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | STEP [300/340] | CE Loss 1.0182\n",
            "EPOCH [6/30] | STEP [300/340] | ADV Loss 1.4979\n",
            "EPOCH [6/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [6/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [6/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [6/30] | Current F1-Macro 50.79\n",
            "EPOCH [6/30] | Best F1-Macro 50.79\n",
            "[[ 5  1  0  1  0  0  0  0  0  1  0]\n",
            " [ 2 29  2  1  1  0  1  0  0  0  0]\n",
            " [ 0  0 51  7 10  0 20  5  0  4  5]\n",
            " [ 1  4  7 60 10  8  0  4  0  2  0]\n",
            " [ 2  1  2  4 17  0  2  1  0  0  0]\n",
            " [ 0  0  2 39  4 43  0  1  0  2  0]\n",
            " [ 0  2  7  0  2  1 41  3  0  2  2]\n",
            " [ 0  0  0  1  0  0  0  7  0  0  1]\n",
            " [ 0  0  0  1  1  0  3  1  0  0  1]\n",
            " [ 0  1  0  0  1  0  1  0  0  7  1]\n",
            " [ 1  0  3  1  1  0  7  0  0  4 20]]\n",
            "not early stopping\n",
            "EPOCH [7/30] | STEP [100/340] | CE Loss 0.881\n",
            "EPOCH [7/30] | STEP [100/340] | ADV Loss 1.4077\n",
            "EPOCH [7/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | STEP [200/340] | CE Loss 0.735\n",
            "EPOCH [7/30] | STEP [200/340] | ADV Loss 1.1891\n",
            "EPOCH [7/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | STEP [300/340] | CE Loss 0.7496\n",
            "EPOCH [7/30] | STEP [300/340] | ADV Loss 1.2651\n",
            "EPOCH [7/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [7/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [7/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [7/30] | Current F1-Macro 54.86\n",
            "EPOCH [7/30] | Best F1-Macro 54.86\n",
            "[[ 5  2  0  0  0  1  0  0  0  0  0]\n",
            " [ 0 34  2  0  0  0  0  0  0  0  0]\n",
            " [ 0  1 61  6  4  1 14  5  0  5  5]\n",
            " [ 0  4  9 53  7 16  1  3  0  3  0]\n",
            " [ 1  3  5  2 15  1  1  1  0  0  0]\n",
            " [ 0  1  3 26  2 57  0  1  0  1  0]\n",
            " [ 0  2  8  0  1  1 30  5  2  2  9]\n",
            " [ 0  0  0  1  0  0  0  6  0  0  2]\n",
            " [ 0  0  1  0  1  1  1  1  1  0  1]\n",
            " [ 0  2  1  0  0  0  1  0  0  6  1]\n",
            " [ 0  1  2  1  0  0  3  0  0  4 26]]\n",
            "not early stopping\n",
            "EPOCH [8/30] | STEP [100/340] | CE Loss 0.5879\n",
            "EPOCH [8/30] | STEP [100/340] | ADV Loss 1.0256\n",
            "EPOCH [8/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | STEP [200/340] | CE Loss 0.6102\n",
            "EPOCH [8/30] | STEP [200/340] | ADV Loss 1.0963\n",
            "EPOCH [8/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | STEP [300/340] | CE Loss 0.509\n",
            "EPOCH [8/30] | STEP [300/340] | ADV Loss 1.0446\n",
            "EPOCH [8/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [8/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [8/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [8/30] | Current F1-Macro 58.84\n",
            "EPOCH [8/30] | Best F1-Macro 58.84\n",
            "[[ 5  1  0  1  0  0  0  0  0  1  0]\n",
            " [ 1 31  2  0  2  0  0  0  0  0  0]\n",
            " [ 0  0 64  6  2  1 22  1  3  1  2]\n",
            " [ 0  3 16 59  4 10  4  0  0  0  0]\n",
            " [ 0  2  4  2 17  0  3  1  0  0  0]\n",
            " [ 0  0  4 33  1 52  0  1  0  0  0]\n",
            " [ 0  0  8  0  2  1 39  2  5  0  3]\n",
            " [ 0  0  0  1  0  0  0  6  0  0  2]\n",
            " [ 0  0  0  1  1  0  3  0  2  0  0]\n",
            " [ 0  1  1  0  0  0  2  0  1  4  2]\n",
            " [ 0  1  5  1  0  0  6  0  1  1 22]]\n",
            "not early stopping\n",
            "EPOCH [9/30] | STEP [100/340] | CE Loss 0.456\n",
            "EPOCH [9/30] | STEP [100/340] | ADV Loss 0.8822\n",
            "EPOCH [9/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | STEP [200/340] | CE Loss 0.4532\n",
            "EPOCH [9/30] | STEP [200/340] | ADV Loss 0.9395\n",
            "EPOCH [9/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | STEP [300/340] | CE Loss 0.396\n",
            "EPOCH [9/30] | STEP [300/340] | ADV Loss 0.8579\n",
            "EPOCH [9/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [9/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [9/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [9/30] | Current F1-Macro 52.26\n",
            "EPOCH [9/30] | Best F1-Macro 58.84\n",
            "[[ 5  1  0  1  0  0  0  0  0  1  0]\n",
            " [ 5 26  3  0  2  0  0  0  0  0  0]\n",
            " [ 0  0 71 11  4  1  9  1  0  2  3]\n",
            " [ 1  3 11 66  5  8  1  0  0  1  0]\n",
            " [ 2  1  5  3 16  1  1  0  0  0  0]\n",
            " [ 0  0  2 41  1 43  0  1  0  2  1]\n",
            " [ 0  1 20  0  2  1 26  2  3  1  4]\n",
            " [ 0  0  2  1  0  0  0  4  0  0  2]\n",
            " [ 0  0  1  1  1  0  2  0  1  0  1]\n",
            " [ 0  1  2  0  0  0  1  0  0  6  1]\n",
            " [ 1  0  8  1  0  0  2  0  0  4 21]]\n",
            "not early stopping\n",
            "EPOCH [10/30] | STEP [100/340] | CE Loss 0.3346\n",
            "EPOCH [10/30] | STEP [100/340] | ADV Loss 0.7177\n",
            "EPOCH [10/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | STEP [200/340] | CE Loss 0.4053\n",
            "EPOCH [10/30] | STEP [200/340] | ADV Loss 0.8393\n",
            "EPOCH [10/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | STEP [300/340] | CE Loss 0.2844\n",
            "EPOCH [10/30] | STEP [300/340] | ADV Loss 0.6324\n",
            "EPOCH [10/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [10/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [10/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [10/30] | Current F1-Macro 55.6\n",
            "EPOCH [10/30] | Best F1-Macro 58.84\n",
            "[[ 5  2  0  0  0  1  0  0  0  0  0]\n",
            " [ 2 31  1  0  2  0  0  0  0  0  0]\n",
            " [ 0  3 72  8  3  2  8  1  0  1  4]\n",
            " [ 1  3 15 53  4 18  2  0  0  0  0]\n",
            " [ 2  1  4  1 16  4  1  0  0  0  0]\n",
            " [ 0  0  3 19  1 65  0  1  0  1  1]\n",
            " [ 0  2 19  0  2  1 27  1  2  1  5]\n",
            " [ 0  0  2  0  0  1  1  3  0  0  2]\n",
            " [ 0  0  1  0  1  1  2  0  1  0  1]\n",
            " [ 0  2  1  0  0  0  1  0  0  6  1]\n",
            " [ 0  1  6  1  0  0  1  0  0  3 25]]\n",
            "not early stopping\n",
            "EPOCH [11/30] | STEP [100/340] | CE Loss 0.2844\n",
            "EPOCH [11/30] | STEP [100/340] | ADV Loss 0.6529\n",
            "EPOCH [11/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | STEP [200/340] | CE Loss 0.2624\n",
            "EPOCH [11/30] | STEP [200/340] | ADV Loss 0.607\n",
            "EPOCH [11/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | STEP [300/340] | CE Loss 0.2866\n",
            "EPOCH [11/30] | STEP [300/340] | ADV Loss 0.6619\n",
            "EPOCH [11/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [11/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [11/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [11/30] | Current F1-Macro 56.56\n",
            "EPOCH [11/30] | Best F1-Macro 58.84\n",
            "[[ 5  1  0  1  0  0  0  0  0  1  0]\n",
            " [ 2 30  1  0  2  0  0  0  0  1  0]\n",
            " [ 0  3 62  7  3  1 17  1  0  2  6]\n",
            " [ 1  3 11 60  6 11  3  0  0  1  0]\n",
            " [ 2  1  3  2 18  1  2  0  0  0  0]\n",
            " [ 0  0  2 29  3 54  0  1  0  1  1]\n",
            " [ 0  1  9  1  2  1 39  1  2  2  2]\n",
            " [ 0  0  0  1  0  0  0  7  0  0  1]\n",
            " [ 0  0  1  1  1  0  3  0  0  0  1]\n",
            " [ 0  2  0  0  0  0  2  0  0  6  1]\n",
            " [ 0  1  2  1  1  0  5  0  0  3 24]]\n",
            "not early stopping\n",
            "EPOCH [12/30] | STEP [100/340] | CE Loss 0.2135\n",
            "EPOCH [12/30] | STEP [100/340] | ADV Loss 0.5885\n",
            "EPOCH [12/30] | STEP [100/340] | CON Loss 0.0\n",
            "EPOCH [12/30] | STEP [100/340] | VAT Loss 0.0\n",
            "EPOCH [12/30] | STEP [100/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [12/30] | STEP [200/340] | CE Loss 0.2243\n",
            "EPOCH [12/30] | STEP [200/340] | ADV Loss 0.5171\n",
            "EPOCH [12/30] | STEP [200/340] | CON Loss 0.0\n",
            "EPOCH [12/30] | STEP [200/340] | VAT Loss 0.0\n",
            "EPOCH [12/30] | STEP [200/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [12/30] | STEP [300/340] | CE Loss 0.2127\n",
            "EPOCH [12/30] | STEP [300/340] | ADV Loss 0.545\n",
            "EPOCH [12/30] | STEP [300/340] | CON Loss 0.0\n",
            "EPOCH [12/30] | STEP [300/340] | VAT Loss 0.0\n",
            "EPOCH [12/30] | STEP [300/340] | UL Loss 0.0\n",
            "------------------------------------------------\n",
            "EPOCH [12/30] | Current F1-Macro 55.69\n",
            "EPOCH [12/30] | Best F1-Macro 58.84\n",
            "[[ 5  1  0  0  0  1  0  0  0  1  0]\n",
            " [ 1 31  1  0  2  0  0  0  0  1  0]\n",
            " [ 0  1 71  8  2  1 11  1  0  3  4]\n",
            " [ 1  3 16 61  0 12  2  0  0  1  0]\n",
            " [ 2  1  7  2 15  1  1  0  0  0  0]\n",
            " [ 0  0  3 27  1 57  0  1  0  1  1]\n",
            " [ 0  1 12  1  2  1 34  2  2  2  3]\n",
            " [ 0  0  2  1  0  0  1  3  0  0  2]\n",
            " [ 0  0  1  1  1  0  2  0  1  0  1]\n",
            " [ 0  1  0  2  0  0  1  0  0  6  1]\n",
            " [ 0  1  7  1  0  0  3  0  0  3 22]]\n",
            "not early stopping\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}